#By Josef Meile <jmeile@hotmail.com>
#This will block bad bots in your Virtual Hosts (ie: download managers)
#First you just have to include it in your virtual host:
#Include ${EVIL_HOST_PATH}/block_bots.conf

#Then add this after the include (uncomment it):
#<RequireAll>
#  Require all granted
#  Require not env BLACK_LISTED_BOT
#</RequireAll>
#this will grant the access to everybody, except if it was listed as a
#bad bot.

#If you want to restrict access by ips, but you still don't trust the
#people connecting there, then you can do this:
#<RequireAny>
#  Require local
#  <RequireAll>
#    <RequireAny>
#      #Unique ip address
#      Require ip 10.1.2.3
#      #Network/netmask pair
#      Require ip 10.1.0.0/255.255.0.0
#      #Network/nnn CIDR specification:
#      Require ip 10.1.0.0/16
#    </RequireAny>
#    Require not env BLACK_LISTED_BOT
#  </RequireAll>
#</RequireAny>

#What it does:
#- If the site is accessed from the local network: 127.0.0.0/8 or ::1, then it
#  will grant the access
#- If it is not the local network, then it will continue evaluating the
#  "RequireAll" directive as follows:
#  If the client address matches any of the ips listed in the "Require ip"
#  directive and it wasn't listed as a bad bot, then the access will be granted.

#An easier way of understanding the RequireAny and RequireAll blocks is to see
#them as logical "Or" and "And" operators (respectively), so, for the example
#above, the condition would be:
#(ip in local) or
#	(
#		(
#			(ip == 10.1.2.3) or (ip in 10.1.0.0/255.255.0.0) or
#			(ip in 10.1.0.0/16)
#		) and (not BLACK_LISTED_BOT)
#	)

#If using this with software like dokuwiki, do not forget to put
#AllowOverride All
#in your DocumentRoot directory; otherwise all the "Require" directives in the
#preceeding subfolders and/or .htaccess files will default to "Require all
#granted"

#Alternatively, you can include it either in your global apache .conf file or
#put it in the configuration files folder, ie:
#/etc/apache2/conf-enabled -> In devian/ubuntu like distros
#/etc/httpd/conf.d         -> In RedHad/CentOS like distros

#I opted to linking it into the configuration folder, ie:
#cd /etc/apache2/conf-enabled
#ln -s /usr/local/apache_evil_host_trap/block_bots.conf .

#The blocked accesses will be logged to:
#/var/log/apache2/blocked_user_agents.log

#If your bad bots list is big, then you can split it on serveral lines,
#just add a "BrowserMatchNoCase ... BLACK_LISTED_BOT" per line

<Location />
   BrowserMatchNoCase (httrack|scraper|copier|collector|mail|grabber) BLACK_LISTED_BOT
   BrowserMatchNoCase (flashget|harvest|loader|walker|scan|record) BLACK_LISTED_BOT
   BrowserMatchNoCase (download|reget|siphon|xenu|sucker|stripper|extract|java) BLACK_LISTED_BOT

   #Bots which doesn't seem to be bad, but match the rules above, so, they are excluded here
   BrowserMatchNoCase (mail.ru_bot|archive.org_bot|AndroidDownloadManager) !BLACK_LISTED_BOT
</Location>
CustomLog ${APACHE_LOG_DIR}/blocked_user_agents.log combined env=BLACK_LISTED_BOT
